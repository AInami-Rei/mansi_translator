# Обучение модели mbart

# Улучшение токенизатора
- Инициализация весов в токенайзере через разные языки (реализовано сочетание - финский, русский, венгерский)
- Использовать не только тексты, но и словари для обучения токенизатора

# Модель
- эксперименты с количеством эпох и “классическими“ гиперпараметрами
- расширение обучающей выборки
- можно обучать с lora адаптерами

# Evaluation
- постобработка ответов - часто предсказывается без долгот, но в остальном всё верно
- Проработка альтернативных методов оценки качество модели, потому что BLEU не идеален и об этом говорят ещё на конференции в 2022 году https://aclanthology.org/2022.wmt-1.2.pdf
