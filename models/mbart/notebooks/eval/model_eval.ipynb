{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53d4b43-3eab-4bf5-beed-f35b39eccd1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T22:58:58.894542Z",
     "iopub.status.busy": "2024-09-08T22:58:58.893681Z",
     "iopub.status.idle": "2024-09-08T22:58:58.927681Z",
     "shell.execute_reply": "2024-09-08T22:58:58.926503Z",
     "shell.execute_reply.started": "2024-09-08T22:58:58.894488Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9157a8ae-3368-4dfb-bd9b-79ac4b10a526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T22:59:00.302920Z",
     "iopub.status.busy": "2024-09-08T22:59:00.301302Z",
     "iopub.status.idle": "2024-09-08T22:59:08.382909Z",
     "shell.execute_reply": "2024-09-08T22:59:08.381745Z",
     "shell.execute_reply.started": "2024-09-08T22:59:00.302879Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_model(model_path=\"./mbart-large-51-ru-mans-v2-full-finetuneepoch_4/\"):\n",
    "    tokenizer = MBart50Tokenizer.from_pretrained(model_path)\n",
    "    old_len = len(tokenizer)\n",
    "    tokenizer.lang_code_to_id['mans_XX'] = old_len-1\n",
    "    tokenizer.id_to_lang_code[old_len-1] = 'mans_XX'\n",
    "    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n",
    "    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n",
    "    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n",
    "    if 'mans_XX' not in tokenizer._additional_special_tokens:\n",
    "        tokenizer._additional_special_tokens.append('mans_XX')\n",
    "\n",
    "    tokenizer.src_lang = \"ru_RU\"\n",
    "    tokenizer.tgt_lang = \"mans_XX\"\n",
    "    \n",
    "    \n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_path,\n",
    "                                                        torch_dtype=torch.bfloat16,\n",
    "                                                        #attn_implementation=\"flash_attention_2\", #https://huggingface.co/docs/transformers/perf_infer_gpu_one#combine-optimizations\n",
    "                                                        #WORKS ONLY ON AMPERS GPUS\n",
    "                                                        load_in_8bit=True,\n",
    "                                                         )\n",
    "    print(model.device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44bd0eca-b0da-44f6-ac43-829dead2d9bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T22:59:08.386182Z",
     "iopub.status.busy": "2024-09-08T22:59:08.384980Z",
     "iopub.status.idle": "2024-09-08T22:59:08.582648Z",
     "shell.execute_reply": "2024-09-08T22:59:08.581271Z",
     "shell.execute_reply.started": "2024-09-08T22:59:08.386141Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = model.to_bettertransformer()\n",
    "\n",
    "model = torch.compile(model) #https://huggingface.co/docs/transformers/perf_torch_compile#v100-batch-size-1\n",
    "# https://habr.com/ru/companies/wunderfund/articles/820721/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b857350-1d46-4b8b-adea-f39b6f63dc1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T22:59:20.263890Z",
     "iopub.status.busy": "2024-09-08T22:59:20.262380Z",
     "iopub.status.idle": "2024-09-08T22:59:20.298214Z",
     "shell.execute_reply": "2024-09-08T22:59:20.297099Z",
     "shell.execute_reply.started": "2024-09-08T22:59:20.263851Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate(text, src='ru_RU', trg='mans_XX', max_length=200, num_beams=5, repetition_penalty=5.0, **kwargs):\n",
    "    tokenizer.src_lang = src\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # # enable FlashAttention\n",
    "    # with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "    ### works ONLY ON AMPER GPUS\n",
    "    \n",
    "    generated_tokens = model.generate(\n",
    "        **encoded.to(model.device),\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[trg], \n",
    "        max_length=max_length, \n",
    "        num_beams=num_beams,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        # early_stopping=True,\n",
    "    )\n",
    "        \n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "def translate_batch(texts, src='ru_RU', trg='mans_XX', max_length=200, num_beams=5, repetition_penalty=5.0,\n",
    "                    batch_size=128, **kwargs):\n",
    "    tokenizer.src_lang = src\n",
    "    all_translations = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        encoded = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # # enable FlashAttention\n",
    "        # with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "        ### works ONLY ON AMPER GPUS\n",
    "        \n",
    "        generated_tokens = model.generate(\n",
    "            **{k: v.to(model.device) for k, v in encoded.items()},\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[trg],\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            # early_stopping=True,\n",
    "        )\n",
    "        \n",
    "        batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        all_translations.extend(batch_translations)\n",
    "    \n",
    "    return all_translations\n",
    "\n",
    "def translate_dataframe(df, text_column, src='ru_RU', trg='mans_XX', **kwargs):\n",
    "    texts = df[text_column].tolist()\n",
    "    translations = translate_batch(texts, src=src, trg=trg, **kwargs)\n",
    "    return pd.DataFrame({'original': texts, 'translation_model': translations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "412ce495-310a-42b0-bbcf-adb734116cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T22:59:23.000329Z",
     "iopub.status.busy": "2024-09-08T22:59:22.998955Z",
     "iopub.status.idle": "2024-09-08T22:59:30.940771Z",
     "shell.execute_reply": "2024-09-08T22:59:30.939554Z",
     "shell.execute_reply.started": "2024-09-08T22:59:23.000265Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 22:59:23.959738: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-08 22:59:25.142866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading builder script: 100%|██████████| 9.01k/9.01k [00:00<00:00, 20.9MB/s]\n",
      "Downloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 15.4MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 4.91MB/s]                   \n",
      "Downloading extra modules: 100%|██████████| 3.34k/3.34k [00:00<00:00, 11.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def get_simple_metrics(data: pd.DataFrame, preds_column: str,\n",
    "                       original_column: str, output_file: str):\n",
    "    \n",
    "    chrf__ = chrf.compute(\n",
    "        predictions=data[preds_column].values,\n",
    "        references=data[original_column].values,\n",
    "        word_order=2,\n",
    "    )[\"score\"]\n",
    "    bleu__ = bleu.compute(\n",
    "        predictions=data[preds_column].values, references=data[original_column].values\n",
    "    )[\"bleu\"]\n",
    "    metrics = pd.DataFrame(\n",
    "        {\"chrf\": [chrf__], \"bleu\": [bleu__]}\n",
    "    )\n",
    "    print(output_file, f\"bleu {bleu__}\", f\"chrf {chrf__}\")\n",
    "    metrics.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d4f477f-7aa1-4bcd-b636-f7dea32b9674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T22:59:30.944206Z",
     "iopub.status.busy": "2024-09-08T22:59:30.942968Z",
     "iopub.status.idle": "2024-09-08T22:59:31.010933Z",
     "shell.execute_reply": "2024-09-08T22:59:31.009785Z",
     "shell.execute_reply.started": "2024-09-08T22:59:30.944089Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: 8821\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "model_names = [\"mbart-large-51-ru-mans-v2-full-finetuneepoch_4\",\n",
    "              \"mbart-large-51-ru-mans-v2-full-finetuneepoch_3\",\n",
    "              \"mbart-large-51-ru-mans-v2-full-finetuneepoch_2\",\n",
    "              \"mbart-large-51-ru-mans-v2-full-finetuneepoch_1\",\n",
    "              \"mbart-large-51-ru-mans-v2-full-finetuneepoch_0\"]\n",
    "\n",
    "test = pd.read_csv('./test.csv')\n",
    "print(f\"test size: {test.shape[0]}\")\n",
    "\n",
    "def run_model_test(model_name):\n",
    "    translated_test = translate_dataframe(test, \"ru\")\n",
    "    \n",
    "    #save results\n",
    "    translated_test.to_csv(f'./metrics/test_predicted_{model_name}.csv')\n",
    "        \n",
    "        \n",
    "    #compute metrics\n",
    "    get_simple_metrics(translated_test, preds_column=f\"predicted_{model_name}\",\n",
    "                       original_column=\"mans\", output_file=f\"./metrics/results_{model_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "219263aa-8818-4a13-93d3-ad93a7b38e1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T22:59:31.013427Z",
     "iopub.status.busy": "2024-09-08T22:59:31.012460Z",
     "iopub.status.idle": "2024-09-08T22:59:32.502950Z",
     "shell.execute_reply": "2024-09-08T22:59:32.501746Z",
     "shell.execute_reply.started": "2024-09-08T22:59:31.013378Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Девушка Куринька мастерит.', 'А̄ги Кӯринька ва̄ранты.'],\n",
       "       dtype=object),\n",
       " 'А̄ги Куринька ма̄щтыр.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simple test\n",
    "\n",
    "test.values[3], translate(test.values[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dcc5e8d-d72b-4b7c-93e4-e6378521da53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T22:59:32.506516Z",
     "iopub.status.busy": "2024-09-08T22:59:32.505663Z",
     "iopub.status.idle": "2024-09-08T22:59:34.988773Z",
     "shell.execute_reply": "2024-09-08T22:59:34.987452Z",
     "shell.execute_reply.started": "2024-09-08T22:59:32.506482Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrf {'score': 34.60185689354383, 'char_order': 6, 'word_order': 2, 'beta': 2}\n",
      "bleu {'bleu': 0.0, 'precisions': [0.5, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 4, 'reference_length': 4}\n"
     ]
    }
   ],
   "source": [
    "chrf = evaluate.load(\"chrf\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "    \n",
    "print('chrf', chrf.compute(predictions=['А̄ги Куринька ма̄щтыр.'], references=['А̄ги Кӯринька ва̄ранты.'], word_order=2))\n",
    "\n",
    "print('bleu', bleu.compute(predictions=['А̄ги Куринька ма̄щтыр.'], references=['А̄ги Кӯринька ва̄ранты.']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72398743-bbd4-4155-9af3-0e649bfe232b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T23:24:22.321723Z",
     "iopub.status.busy": "2024-09-08T23:24:22.319926Z",
     "iopub.status.idle": "2024-09-08T23:24:27.827688Z",
     "shell.execute_reply": "2024-09-08T23:24:27.826339Z",
     "shell.execute_reply.started": "2024-09-08T23:24:22.321661Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./metrics/results_mbart_v2_4epochs.csv bleu 0.16135040463809533 chrf 42.84673235429147\n"
     ]
    }
   ],
   "source": [
    "# run_model_test(model_names[0])\n",
    "\n",
    "translated_test = pd.read_csv('./metrics/test_predicted_mbart-large-51-ru-mans-v2-full-finetuneepoch_4.csv')\n",
    "test['model_translation'] = translated_test['translation']\n",
    "\n",
    "#compute metrics\n",
    "get_simple_metrics(test, preds_column=\"model_translation\",\n",
    "                    original_column=\"mans\", output_file=f\"./metrics/results_mbart_v2_4epochs.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
